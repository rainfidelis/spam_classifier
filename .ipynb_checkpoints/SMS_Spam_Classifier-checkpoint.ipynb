{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24ab31e",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "Spam detection is a major application of Machine Learning today. Mostly used by email providers, spam detection systems enable them catch fishy and potentially fraudulent emails, typically sending them over to 'Junk Mail' or 'Spam' folders.\n",
    "\n",
    "In this project, I will be building a spam detection classifier for SMS messages using the Naive Bayes algorithm. The classifier should be able to look at an SMS and classify it either as spam or as 'ham' (a category reserved for messages that are not spam). \n",
    "\n",
    "But what constitutes a spam message? They're often unsolicited messages mass-sent to individuals for advertising, phishing, or other fraudulent purposes. Designed to be enticing, spam messages often contain words like 'win', 'winner', 'free', 'cash', 'prize', etc. to ensure you open them. They may also use a lot of exclamation marks or be written in all-caps to make them noticeable.\n",
    "\n",
    "The goal of this project is to build a model that correctly identifies these messages, making it easier to filter them out and keep them from getting to unsuspecting recipients. This is a binary classification problem where each message will be classified as 'Spam' or 'Ham' (Not Spam). The resulting model should be useful for predicting spam messages on data it hasn't \"seen\" before.\n",
    "\n",
    "\n",
    "## The Data\n",
    "\n",
    "The data for this project is a collection of SMS messages from mobile phones made publicly available by their recipients for research purposes. The data is available for download on the UCI Machine Learning Repository, and can be found [here](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). \n",
    "\n",
    "For this project, the file has been downloaded and is named `SMSSpamCollection`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5373fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Packages for preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Model packages\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import(AdaBoostClassifier,\n",
    "                             RandomForestClassifier,\n",
    "                             BaggingClassifier)\n",
    "\n",
    "# Packages for Performance Measurement\n",
    "from sklearn.metrics import(confusion_matrix, \n",
    "                            accuracy_score,\n",
    "                            precision_score,\n",
    "                            recall_score,\n",
    "                            f1_score)\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce376935",
   "metadata": {},
   "source": [
    "## Understanding the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e83604e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the file and display sample values\n",
    "\n",
    "file = 'SMSSpamCollection'\n",
    "data = pd.read_table(file, sep='\\t', names=['label', 'message'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "838a0d3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5572 entries in the dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the balance of the dataset\n",
    "\n",
    "print(f'There are {data.shape[0]} entries in the dataset')\n",
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf3e689",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAIN\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEHCAYAAABfkmooAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAARsUlEQVR4nO3dfZBdd13H8feHtJQKFNvptpZsNR2Mjm0RMGusMD7wMBJFTUWKYcBmtGOYWgUdR22dEVEnigo+8NDORK1JRa0RxAa1YI2goqVlI4U0LZUMLW1MbAKoFB8qab/+cX+ZXpJtflvcc3fTfb9m7pxzvuecu9/N3Mlnz9PvpqqQJOl4nrDYDUiSlj7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXScN+eZJ7gEeAB4CDlfVTJIzgD8GVgH3AC+vqn9r218FXNa2f01VvbfV1wBbgVOBvwReW517fs8888xatWrVgv9OkvR4tmvXrk9V1dTR9UHDonl+VX1qbPlKYGdVvSHJlW35p5OcD2wALgCeDvx1kq+qqoeAa4BNwAcZhcU64Mbj/dBVq1YxOzu78L+NJD2OJfnkXPXFOA21HtjW5rcBF4/Vr6+qB6vqbmAvsDbJOcBpVXVzO5q4bmwfSdIEDB0WBfxVkl1JNrXa2VV1AKBNz2r1lcB9Y/vua7WVbf7o+jGSbEoym2T20KFDC/hrSNLyNvRpqOdV1f4kZwE3JfnYcbbNHLU6Tv3YYtUWYAvAzMyM45hI0gIZ9Miiqva36UHgXcBa4P52aok2Pdg23wecO7b7NLC/1afnqEuSJmSwsEjy5CRPPTIPfBtwO7AD2Ng22wjc0OZ3ABuSnJLkPGA1cGs7VfVAkouSBLh0bB9J0gQMeRrqbOBdo//fOQn4w6p6T5IPAduTXAbcC1wCUFV7kmwH7gAOA1e0O6EALueRW2dvpHMnlCRpYeXxOkT5zMxMeeusJD02SXZV1czRdZ/gliR1GRaSpK5JPMF9Qlrzk9ctdgtagnb92qWL3YK0KDyykCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYOHRZIVST6c5M/b8hlJbkry8TY9fWzbq5LsTXJXkheP1dck2d3WvTlJhu5bkvSISRxZvBa4c2z5SmBnVa0GdrZlkpwPbAAuANYBVydZ0fa5BtgErG6vdRPoW5LUDBoWSaaBlwC/M1ZeD2xr89uAi8fq11fVg1V1N7AXWJvkHOC0qrq5qgq4bmwfSdIEDH1k8ZvATwEPj9XOrqoDAG16VquvBO4b225fq61s80fXj5FkU5LZJLOHDh1akF9AkjRgWCT5TuBgVe2a7y5z1Oo49WOLVVuqaqaqZqampub5YyVJPScN+N7PA747yXcATwJOS/J24P4k51TVgXaK6WDbfh9w7tj+08D+Vp+eoy5JmpDBjiyq6qqqmq6qVYwuXP9NVb0K2AFsbJttBG5o8zuADUlOSXIeowvZt7ZTVQ8kuajdBXXp2D6SpAkY8sji0bwB2J7kMuBe4BKAqtqTZDtwB3AYuKKqHmr7XA5sBU4FbmwvSdKETCQsqur9wPvb/KeBFz7KdpuBzXPUZ4ELh+tQknQ8PsEtSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2DhUWSJyW5NclHkuxJ8vOtfkaSm5J8vE1PH9vnqiR7k9yV5MVj9TVJdrd1b06SofqWJB1ryCOLB4EXVNWzgGcD65JcBFwJ7Kyq1cDOtkyS84ENwAXAOuDqJCvae10DbAJWt9e6AfuWJB1lsLCokc+1xZPbq4D1wLZW3wZc3ObXA9dX1YNVdTewF1ib5BzgtKq6uaoKuG5sH0nSBAx6zSLJiiS3AQeBm6rqFuDsqjoA0KZntc1XAveN7b6v1Va2+aPrc/28TUlmk8weOnRoQX8XSVrOBg2Lqnqoqp4NTDM6SrjwOJvPdR2ijlOf6+dtqaqZqpqZmpp6zP1KkuY2kbuhqurfgfczutZwfzu1RJsebJvtA84d220a2N/q03PUJUkTMuTdUFNJvrTNnwq8CPgYsAPY2DbbCNzQ5ncAG5KckuQ8Rheyb22nqh5IclG7C+rSsX0kSRNw0oDvfQ6wrd3R9ARge1X9eZKbge1JLgPuBS4BqKo9SbYDdwCHgSuq6qH2XpcDW4FTgRvbS5I0IYOFRVV9FHjOHPVPAy98lH02A5vnqM8Cx7veIUkakE9wS5K6DAtJUpdhIUnqmldYJNk5n5ok6fHpuBe4kzwJ+BLgzDbg35EH5E4Dnj5wb5KkJaJ3N9SrgR9jFAy7eCQsPgu8bbi2JElLyXHDoqp+C/itJD9aVW+ZUE+SpCVmXs9ZVNVbkjwXWDW+T1VdN1BfkqQlZF5hkeT3gWcAtwFHnqo+Mly4JOlxbr5PcM8A57fvk5AkLTPzfc7iduDLhmxEkrR0zffI4kzgjiS3Mvq6VACq6rsH6UqStKTMNyxeP2QTkqSlbb53Q/3t0I1Ikpau+d4N9QCPfJXpE4GTgf+sqtOGakyStHTM98jiqePLSS4G1g7RkCRp6fmiRp2tqj8DXrCwrUiSlqr5noZ66djiExg9d+EzF5K0TMz3bqjvGps/DNwDrF/wbiRJS9J8r1n8wNCNSJKWrvl++dF0knclOZjk/iTvTDI9dHOSpKVhvhe4fw/Yweh7LVYC7241SdIyMN+wmKqq36uqw+21FZgasC9J0hIy37D4VJJXJVnRXq8CPj1kY5KkpWO+YfGDwMuBfwUOAC8DvOgtScvEfG+d/UVgY1X9G0CSM4A3MgoRSdLj3HyPLL72SFAAVNVngOcM05IkaamZb1g8IcnpRxbakcV8j0okSSe4+f6H/ybgH5O8g9EwHy8HNg/WlSRpSZnvE9zXJZllNHhggJdW1R2DdiZJWjLmfSqphYMBIUnL0Bc1RLkkaXkxLCRJXYaFJKlrsLBIcm6S9yW5M8meJK9t9TOS3JTk4206fkvuVUn2JrkryYvH6muS7G7r3pwkQ/UtSTrWkEcWh4GfqKqvAS4CrkhyPnAlsLOqVgM72zJt3QbgAmAdcHWSFe29rgE2Aavba92AfUuSjjJYWFTVgar6pzb/AHAno+HN1wPb2mbbgIvb/Hrg+qp6sKruBvYCa5OcA5xWVTdXVQHXje0jSZqAiVyzSLKK0fAgtwBnV9UBGAUKcFbbbCVw39hu+1ptZZs/uj7Xz9mUZDbJ7KFDhxb0d5Ck5WzwsEjyFOCdwI9V1WePt+kctTpO/dhi1Zaqmqmqmakpv25DkhbKoGGR5GRGQfEHVfWnrXx/O7VEmx5s9X3AuWO7TwP7W316jrokaUKGvBsqwO8Cd1bVr4+t2gFsbPMbgRvG6huSnJLkPEYXsm9tp6oeSHJRe89Lx/aRJE3AkCPHPg/4fmB3ktta7WeANwDbk1wG3AtcAlBVe5JsZzSkyGHgiqp6qO13ObAVOBW4sb0kSRMyWFhU1QeY+3oDwAsfZZ/NzDGabVXNAhcuXHeSpMfCJ7glSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqWuwsEhybZKDSW4fq52R5KYkH2/T08fWXZVkb5K7krx4rL4mye627s1JMlTPkqS5DXlksRVYd1TtSmBnVa0GdrZlkpwPbAAuaPtcnWRF2+caYBOwur2Ofk9J0sAGC4uq+jvgM0eV1wPb2vw24OKx+vVV9WBV3Q3sBdYmOQc4rapurqoCrhvbR5I0IZO+ZnF2VR0AaNOzWn0lcN/YdvtabWWbP7o+pySbkswmmT106NCCNi5Jy9lSucA913WIOk59TlW1papmqmpmampqwZqTpOVu0mFxfzu1RJsebPV9wLlj200D+1t9eo66JGmCJh0WO4CNbX4jcMNYfUOSU5Kcx+hC9q3tVNUDSS5qd0FdOraPJGlCThrqjZP8EfCtwJlJ9gE/B7wB2J7kMuBe4BKAqtqTZDtwB3AYuKKqHmpvdTmjO6tOBW5sL0nSBA0WFlX1ikdZ9cJH2X4zsHmO+ixw4QK2Jkl6jJbKBW5J0hJmWEiSugwLSVKXYSFJ6jIsJEldg90NJWk49/7CMxe7BS1BX/663YO9t0cWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUtcJExZJ1iW5K8neJFcudj+StJycEGGRZAXwNuDbgfOBVyQ5f3G7kqTl44QIC2AtsLeqPlFV/wtcD6xf5J4kadk4abEbmKeVwH1jy/uAbzh6oySbgE1t8XNJ7ppAb8vBmcCnFruJpSBv3LjYLehYfj6P+LksxLt8xVzFEyUs5voXqGMKVVuALcO3s7wkma2qmcXuQ5qLn8/JOFFOQ+0Dzh1bngb2L1IvkrTsnChh8SFgdZLzkjwR2ADsWOSeJGnZOCFOQ1XV4SQ/ArwXWAFcW1V7Frmt5cRTe1rK/HxOQKqOOfUvSdIXOFFOQ0mSFpFhIUnqMiyWsSSrkty+2H1IWvoMC0lSl2GhFUl+O8meJH+V5NQkP5TkQ0k+kuSdSb4EIMnWJNckeV+STyT5liTXJrkzydZF/j30OJDkyUn+on32bk/yfUnuSfIrSW5tr69s235XkluSfDjJXyc5u9Vfn2Rb+zzfk+SlSX41ye4k70ly8uL+licmw0KrgbdV1QXAvwPfC/xpVX19VT0LuBO4bGz704EXAD8OvBv4DeAC4JlJnj3BvvX4tA7YX1XPqqoLgfe0+merai3wVuA3W+0DwEVV9RxG48X91Nj7PAN4CaMx5N4OvK+qngn8d6vrMTIsdHdV3dbmdwGrgAuT/H2S3cArGYXBEe+u0f3Wu4H7q2p3VT0M7Gn7Sv8fu4EXtSOJb6qq/2j1PxqbfmObnwbe2z6nP8kXfk5vrKrPt/dbwSOhsxs/p18Uw0IPjs0/xOhBza3Aj7S/xH4eeNIc2z981L4Pc4I85Kmlq6r+GVjD6D/1X07yuiOrxjdr07cAb22f01czx+e0/SHz+XrkgTI/p18kw0JzeSpwoJ3bfeViN6PlI8nTgf+qqrcDbwS+rq36vrHpzW3+acC/tHmHAx6YCau5/CxwC/BJRn/hPXVx29Ey8kzg15I8DHweuBx4B3BKklsY/YH7irbt64E/SfIvwAeB8ybf7vLhcB+SlrQk9wAzVeV3ViwiT0NJkro8spAkdXlkIUnqMiwkSV2GhSSpy7CQFkCSz3XWP+YRfttYXC/7/3UmLQzDQpLUZVhICyjJU5LsTPJPbZTT9WOrT2qjoX40yTvGRvNdk+Rvk+xK8t4k5yxS+9KjMiykhfU/wPdU1dcBzwfelCRt3VcDW6rqa4HPAj/chlR5C/CyqloDXAtsXoS+peNyuA9pYQX4pSTfzGjQupXA2W3dfVX1D23+7cBrGI2GeiFwU8uUFcCBiXYszYNhIS2sVwJTwJqq+nwbquLIaKhHPwFbjMJlT1V9I9IS5mkoaWE9DTjYguL5wFeMrfvyJEdC4RWMvrznLmDqSD3JyUkuQFpiDAtpYf0BMJNkltFRxsfG1t0JbEzyUeAM4Jqq+l/gZcCvJPkIcBvw3Mm2LPU5NpQkqcsjC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1PV/s02S8sabWfwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the data balance\n",
    "\n",
    "sns.countplot(data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567563b8",
   "metadata": {},
   "source": [
    "There are 747 spam labelled spam messages in the dataset. This is just 13% of the entire dataset, and is fairly similar to real world data where spam messages are typically in the minority. Because of its similarity to real world data, I wouldn't proceed be attempting to alter the balance during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01696035",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "There are three stages to processing the data for use in the machine learning algorithm: \n",
    "\n",
    "+ First, identify and handle missing data if any exist in our dataset\n",
    "+ Next, encode the labels to numeric values with 'ham' as 0 and 'spam' as 1\n",
    "+ Then, split the data into training and testing sets\n",
    "+ Finally, prepare the message column for the machine learning model by creating a bag of words that counts the number of times each word appears in a message. This numeric data is what will be used by the model for prediction, and it can be generated using sklearn's CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8426b646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "message    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of missing data in the dataset (by column)\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c45ec22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    0\n",
       "Name: label, dtype: uint8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the data labels \n",
    "\n",
    "data['label'] = pd.get_dummies(data['label'], drop_first=True)\n",
    "data['label'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "816d80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into test and train groups\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['message'], data['label'], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0721a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag of words for encoding message values using CountVectorizer\n",
    "\n",
    "count_vector = CountVectorizer()\n",
    "X_train = count_vector.fit_transform(X_train) # Transform and fit the training data to the vectorizer\n",
    "X_test = count_vector.transform(X_test) # Transforming the test data without fitting it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307614d",
   "metadata": {},
   "source": [
    "## Model Building - Naive Bayes\n",
    "\n",
    "The data is finally ready to be fed into the Machine Learning model. For this project, I originally used sklearn's Multinomial Naive Bayes algorithm. This is meant to take advantage of the Naive Bayes algorithm's ability to handle the large number of features generated by the count vectorizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2666ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the naive bayes model using default hyperparameters\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = naive_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866aed9c",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "To evaluate the performance of the model, I'd be employing the accuracy, precision, recall, and f1 scores of the model's prediction. The values range from 0 to 1, with 1 being an indication of a perfect performance.\n",
    "\n",
    "To begin, let us visualize the model's performance using a confusion matrix.\n",
    "\n",
    "NB: For a spam classifier, the most important metric is the precision score. Although it's be great to catch all spam messages, it'll be more important that only spam messages are classified as spam by the classifier. This is necessary so that important ham messages are not missed by the recipient because they were misclassified as spam.\n",
    "\n",
    "This means, the model should be optimized to only classsify spam messages as spam, even if that means some spam messages slip through into the ham category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e19c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Ham</th>\n",
       "      <th>Predicted Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Ham</th>\n",
       "      <td>1203</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Spam</th>\n",
       "      <td>11</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Predicted Ham  Predicted Spam\n",
       "Actual Ham            1203               5\n",
       "Actual Spam             11             174"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a confusion matrix to visualize model performance\n",
    "conf_array = confusion_matrix(y_test, y_pred)\n",
    "# Cast the array into a pandas dataframe\n",
    "conf_matrix = pd.DataFrame(conf_array, columns=['Predicted Ham', 'Predicted Spam'], index=['Actual Ham', 'Actual Spam'])\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2f9f519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy score of the model is: 0.9885139985642498\n",
      "The Precision score of the model is: 0.9720670391061452\n",
      "The Recall score of the model is: 0.9405405405405406\n",
      "The F1 score of the model is: 0.9560439560439562\n"
     ]
    }
   ],
   "source": [
    "print(f\"The Accuracy score of the model is: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"The Precision score of the model is: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"The Recall score of the model is: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"The F1 score of the model is: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c49e5b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The model correctly classified 1203 'ham' messages, while allowing 11 spam messages slip through due to misclassification. On the other hand, 174 spam messages were correctly classified as spam, with only 5 ham messages entering the spam folder. This is a great performance on data not previously seen by the model. Also impressive is the fact that hyperparameter tuning is not required for this model.\n",
    "\n",
    "Per our evaluation metrics:\n",
    "+ 98.8% of messages were labelled correctly; this is the **Accuracy score**.\n",
    "+ 97% of messages classified as spam messages were actually spam messages; this is the **Precision score**.\n",
    "+ 94% of spam messages were correctly identified as spam; this is the **Recall score**.\n",
    "\n",
    "The overall implication of this is that our model performed very well on the testing data. For the all important precision score, we got a healthy 97%. This means only 3% of messages classified as spam turned out to be ham messages. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36878ca5",
   "metadata": {},
   "source": [
    "## Model Building - Ensemble Classifiers\n",
    "\n",
    "This section attempts to use ensemble models to improve the evaluation metrics. The goal is to get the precision score to 100% if possible, without badly hurting the recall and accuracy scores. This next step is attempted using the Ada Boost, Random Forest, and Bagging Classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a9326d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the models using ensemble algorithms with 200 learners\n",
    "bag = BaggingClassifier(n_estimators=200)\n",
    "forest = RandomForestClassifier(n_estimators=200)\n",
    "boost = AdaBoostClassifier(n_estimators=200, learning_rate=0.2)\n",
    "\n",
    "# Fit the models to the training data\n",
    "bag.fit(X_train, y_train)\n",
    "forest.fit(X_train, y_train)\n",
    "boost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for each model\n",
    "bag_pred = bag.predict(X_test)\n",
    "forest_pred = forest.predict(X_test)\n",
    "boost_pred = boost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ee32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_true, y_preds, model_name=None):\n",
    "    \"\"\"\n",
    "    A function to calculate and print the model evaluation metrics, given a model and its predictions.\n",
    "    \n",
    "    INPUT:\n",
    "    y_true (Numpy array): the original y values for the test dataset\n",
    "    y_preds (Numpy array): the predicted y values by the model\n",
    "    model_name (str - optional): the name to be associated with the model\n",
    "    \"\"\"\n",
    "    if model_name==None: # print the metrics without a recognized model name\n",
    "        print(f\"The Accuracy score of the model is: {accuracy_score(y_test, y_preds)}\")\n",
    "        print(f\"The Precision score of the model is: {precision_score(y_test, y_preds)}\")\n",
    "        print(f\"The Recall score of the model is: {recall_score(y_test, y_preds)}\")\n",
    "        print(f\"The F1 score of the model is: {f1_score(y_test, y_preds)}\")\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "    else: # acknowledge the classifier name while printing the metrics\n",
    "        print(f\"The Accuracy score of the {model_name} classifier is: {accuracy_score(y_test, y_preds)}\")\n",
    "        print(f\"The Precision score of the {model_name} classifier is: {precision_score(y_test, y_preds)}\")\n",
    "        print(f\"The Recall score of the {model_name} classifier is: {recall_score(y_test, y_preds)}\")\n",
    "        print(f\"The F1 score of the {model_name} classifier is: {f1_score(y_test, y_preds)}\")\n",
    "        print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be13818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy score of the Naive Bayes classifier is: 0.9885139985642498\n",
      "The Precision score of the Naive Bayes classifier is: 0.9720670391061452\n",
      "The Recall score of the Naive Bayes classifier is: 0.9405405405405406\n",
      "The F1 score of the Naive Bayes classifier is: 0.9560439560439562\n",
      "\n",
      "\n",
      "\n",
      "The Accuracy score of the Bagging classifier is: 0.9748743718592965\n",
      "The Precision score of the Bagging classifier is: 0.9120879120879121\n",
      "The Recall score of the Bagging classifier is: 0.8972972972972973\n",
      "The F1 score of the Bagging classifier is: 0.9046321525885558\n",
      "\n",
      "\n",
      "\n",
      "The Accuracy score of the Random Forest classifier is: 0.9820531227566404\n",
      "The Precision score of the Random Forest classifier is: 1.0\n",
      "The Recall score of the Random Forest classifier is: 0.8648648648648649\n",
      "The F1 score of the Random Forest classifier is: 0.927536231884058\n",
      "\n",
      "\n",
      "\n",
      "The Accuracy score of the Ada Boost classifier is: 0.9770279971284996\n",
      "The Precision score of the Ada Boost classifier is: 0.9693251533742331\n",
      "The Recall score of the Ada Boost classifier is: 0.8540540540540541\n",
      "The F1 score of the Ada Boost classifier is: 0.9080459770114943\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print metrics for all the classifiers\n",
    "\n",
    "print_metrics(y_test, y_pred, \"Naive Bayes\")\n",
    "print_metrics(y_test, bag_pred, \"Bagging\")\n",
    "print_metrics(y_test, forest_pred, \"Random Forest\")\n",
    "print_metrics(y_test, boost_pred, \"Ada Boost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5391e259",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Although the naive bayes model performed admirably in all categories, the random forest model was the most precise of them all. 100% of the 160 messsages classified as spam by the random forest classifier were spam messages. However, it had one of the lower recall scores (86%) for all the classifiers, misclassifying 25 spam messages as ham. The model was also very accurate, with an accuracy score of 98%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
